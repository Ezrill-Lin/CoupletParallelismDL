{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"matched_data.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Preprocess the data\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Pad labels to match max_len\n",
    "        label = torch.tensor(\n",
    "            label + [0] * (self.max_len - len(label)), dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "MAX_LEN = 128  # Adjust as needed\n",
    "texts = data[\"Column1\"].tolist()\n",
    "labels = data[\"Column1_Tag\"].apply(lambda x: [int(char) for char in str(x)]).tolist()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SIKU-BERT/sikubert\")\n",
    "sikubert = AutoModel.from_pretrained(\"SIKU-BERT/sikubert\")\n",
    "\n",
    "train_dataset = PoemDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = PoemDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the model\n",
    "class WordSegmentationModel(nn.Module):\n",
    "    def __init__(self, sikubert):\n",
    "        super(WordSegmentationModel, self).__init__()\n",
    "        self.sikubert = sikubert\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(sikubert.config.hidden_size, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.sikubert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WordSegmentationModel(sikubert).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for input_ids, attention_mask, labels in tqdm(train_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                input_ids.to(device),\n",
    "                attention_mask.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in val_loader:\n",
    "                input_ids, attention_mask, labels = (\n",
    "                    input_ids.to(device),\n",
    "                    attention_mask.to(device),\n",
    "                    labels.to(device),\n",
    "                )\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n",
    "# Save the model\n",
    "# torch.save(model, \"word_segmentation_model_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linxi\\AppData\\Local\\Temp\\ipykernel_12144\\3696415269.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"word_segmentation_model_full.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = torch.load(\"word_segmentation_model_full.pth\")\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(test_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                input_ids.to(device),\n",
    "                attention_mask.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=2)  # Get the class with the highest score\n",
    "\n",
    "            # Flatten predictions and labels for evaluation\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "# Prepare the test dataset and loader\n",
    "test_texts = data[\"Column2\"].tolist()  # Replace with actual test texts\n",
    "test_labels = data[\"Column2_Tag\"].apply(lambda x: [int(char) for char in str(x)]).tolist()\n",
    "\n",
    "test_dataset = PoemDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# # Run the testing function\n",
    "# preds, labels = test_model(model, test_loader)\n",
    "\n",
    "# # Filter out padding tokens (assume label -1 is used for padding during training)\n",
    "# valid_preds = [p for p, l in zip(preds, labels) if l != -1]\n",
    "# valid_labels = [l for l in labels if l != -1]\n",
    "\n",
    "# # Evaluate with classification metrics\n",
    "# print(classification_report(valid_labels, valid_preds, target_names=[\"Continuation\", \"Beginning\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to predict tags\n",
    "def predict_tags_batch(texts, model, tokenizer, max_len=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Predict 01 labels for multiple Chinese texts in batch mode.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of input Chinese texts.\n",
    "        model: Trained word segmentation model.\n",
    "        tokenizer: SikuBERT tokenizer.\n",
    "        max_len (int): Maximum sequence length for padding/truncation.\n",
    "        batch_size (int): Number of texts to process in one batch.\n",
    "    \n",
    "    Returns:\n",
    "        list of list of int: Predicted tags (01 labels) for each text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize all texts\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    attention_mask = encodings[\"attention_mask\"]\n",
    "    \n",
    "    # Split data into batches\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    predicted_tags = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            # Get batch data\n",
    "            batch_input_ids = input_ids[i * batch_size : (i + 1) * batch_size].to(device)\n",
    "            batch_attention_mask = attention_mask[i * batch_size : (i + 1) * batch_size].to(device)\n",
    "            \n",
    "            # Predict\n",
    "            outputs = model(batch_input_ids, batch_attention_mask)\n",
    "            batch_predictions = torch.argmax(outputs, dim=2).cpu().numpy()\n",
    "            \n",
    "            # Convert predictions to list of lists (remove padding)\n",
    "            for j, prediction in enumerate(batch_predictions):\n",
    "                text_length = len(texts[i * batch_size + j])\n",
    "                pred = prediction[:text_length].tolist()\n",
    "\n",
    "                if pred[0] == 0:\n",
    "                    pred[0] = 1\n",
    "                predicted_tags.append(pred)\n",
    "                \n",
    "    return predicted_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appllication of the word segmentation model and cosine similarity comparison \n",
    "\n",
    "# load the selected raw couplets texts\n",
    "data = pd.read_csv('filtered_survey_results.csv')\n",
    "text = data.columns[3:].to_list()\n",
    "\n",
    "# Predict the word segmentation tags\n",
    "upper_half_text = []\n",
    "lower_half_text = []\n",
    "for i,string in enumerate(text):\n",
    "    if len(string.split('，')) == 2:\n",
    "        upper_half_text.append(string.split('，')[0])\n",
    "        lower_half_text.append(string.split('，')[1])\n",
    "    else:\n",
    "        upper_half_text.append(string.split(' ')[0])\n",
    "        lower_half_text.append(string.split(' ')[1])\n",
    "survey_df = pd.DataFrame({'Column1': upper_half_text, \n",
    "                          'Column2': lower_half_text})\n",
    "upper_half = predict_tags_batch(upper_half_text, model, tokenizer, batch_size=32)\n",
    "lower_half = predict_tags_batch(lower_half_text, model, tokenizer, batch_size=32)\n",
    "survey_df['Column1_Tag'] = upper_half\n",
    "survey_df['Column2_Tag'] = lower_half\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(row):\n",
    "    array1 = np.array(row[\"Column1_Tag\"])\n",
    "    array2 = np.array(row[\"Column2_Tag\"])\n",
    "    return np.dot(array1, array2) / (np.linalg.norm(array1) * np.linalg.norm(array2))\n",
    "survey_df[\"cosine_similarity\"] = survey_df.apply(cosine_similarity, axis=1)\n",
    "\n",
    "survey_df.to_csv('word_segment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
